<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Chest-xray-classification : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Chest X-ray Classification</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.gatech.edu/shillerman3/chest-xray-classification">View on GitHub</a>

          <h1 id="project_title">Chest X-ray Classification</h1>
          <h2 id="project_tagline">By Steven Hillerman, Leonardo Camacho A, Wyndham Hudson, and Thomas Kim</h2>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
          <h2>Overview</h2>
          <p>This page documents our semester project for Georgia Tech's masters level Machine Learning course in the fall of 2019.  The goal of the project was to create a program capable of taking in X-ray images of a human chest and using those images to determine if the patient has a condition called Pleural Effusion.  According to <a href = "https://medlineplus.gov/ency/article/000086.htm">MedlinePlus.gov</a>, Pleural Effusion occurs when there is an excess amount of fluid built up in the layers of tissue around the lung.  This condition can be visually identified in X-rays because the excess fluid builds up underneath one lung, causing it to be compressed vertically.  Our goal in this project was to create a program that can identify Pleural Effusion automatically using an X-ray image.</p>
          <img src="./images/pleural_effusion_diagram.svg"><br>
          Image Credit to Cancer Research UK

          <h2>Data</h2>
            <p>Our data came from a Stanford project called CheXpert.  The CheXpert project provides a large dataset of labeled X-ray images.  More information on the CheXpert project can be found <a href = "https://stanfordmlgroup.github.io/competitions/chexpert/">here</a>.
            </p>
            <p>While the dataset contained images of a variety of different medical conditions, we elected to focus on only one condition: Pleural Effusion.  We chose Pleural Effusion because there was a large number of positive cases of the condition in the dataset.  Focusing on only one condition allowed us to experiment with multiple methods of classification.  Below are example images for patients with and without Pleural Effusion.
            </p>
            <h4>No Finding:</h4>
              <img src="./images/example_no_finding.jpg"/>
            <h4>Pleural Effusion:</h4>
              <img src="./images/example_pleural_effusion.jpg"/>
            <h4>Data Details</h4>
              <p>The original dataset from CheXpert contained 223,414 images, each classified as either "No Finding" or one of several medical conditions. Because our focus was on Pleural Effusion, removed all data cases that were not either positive Pleural Effusion or positive No Finding.  This cut reduced the size of our dataset to 108,568 images.  Because Pleural Effusion can be best identified from the front, and to prevent the differences in perspective, we also removed all instances of lateral images from the dataset, focusing only on images taken from the front.  This reduced out dataset to its final size of 93,873 images.  While the original dataset contained information about each patient's age and sex, we decided to focus exclusively on the image data and labels.


          <div id="Strategy">
            <h2>Strategy</h2>
            <p>Going into the project, we were not sure what learning algorithm would be best suited for our needs.  Because of this, we decided to try several different classification algorithms and work from those results.  We tried two unsupervised algorithms: K-Means and Gaussian Mixture Model (GMM).  Neither algorithm produced very good results.  We also tried two supervised learning algorithms and one semi-supervised learning algorithm.  The first supervised learning algorithm was Convolutional Neural Networks (CNN).  The second was K-Nearest Neighbors.  Our semi-supervised learning algorithm was a version of Label Propogation called Label Spreading.  Each attempt will be documented in more detail below.
            </p>

            <div id="Algorithms">
              <h3>K-Means and Gaussian Mixture Model (GMM)</h3>
                <p>We used the sci-kit learn python packages for our implementation of K-Means and GMM.  To attempt to capture more complex data, we created 20 different clusters for each algorithm, with the intent that some clusters would be composed of mostly positive Pleural Effusion data while the rest of the clusters were mostly No Finding.  Even with 20 clusters and only the frontal images being considered, both K-Means and GMM struggled to produce any meaningful results.  Our conclusion is that unsupervised clustering is simply not powerful enough to assist in this project without significant data preprocessing to bring out the important features.
                </p>
                <p>Below are the images we reconstructed for the 20 clusters we created, as well as some plots showing the distribution of data throughout the clusters.  As you can clearly see, these approaches produced nothing of interest.</p>
                <h4>K-Means:</h4>
                  <img src="./images/kmeans_cluster_test.png"/>
                  <img src="./images/kmeans results/kmeans_20.png"/>
                <h4>GMM:</h4>
                  <img src="./images/gmm_cluster_test.png"/>
                  <img src="./images/gmm results/gmm_20.png"/>

              <h3>K-Nearest Neighbors (KNN)</h3>
                <h4>Preprocessing</h4>
                  <p> We used a preprocessing algorithm that takes the greyscale X-ray images and converts converts them to only two colors to identify areas that might be lungs. As can be seen in the image below, the lungs are significantly darker than the rest of the body, so the algorithm was able to successfully identify the lung areas to pass to our learning algorithm.  We removed any outliers leave just the lungs in the final image. This process is shown in the image below.
                  </p>
                  <img src="./images/knn_preprocess.png"/>
                <h4>Implementation</h4>
                  <p>We used the sci-kit learn implementation of the K-Nearest Neighbors algorithm. The features were extracted based on the number of pixels making up the left lung, the number of pixels in the right lung, and the total number of pixels in the image. The inspiration behind using K-Nearest Neighbors was that there would be varying differences between the area of the right lung and the area of the left lung if the patient had a pleural effusion or if the patient was healthy. These differences would cluster together and K-Nearest Neighbors would be efficient at determining whether the X-ray showed features more similar to a healthy patient or a patient with a pleural effusion. The number of features and optimal number of neighbors was determined through experimentation, as shown in the image below.
                  </p>
                  <img src="./images/knn_feat_neigh.png"/>
                <h4>Results</h4>
                  <p>The supervised KNN algorithm was a significant improvement over the unsupervised K-Means and GMM algorithms.  The maximum accuracy of the algorithm was found using 125 neighbors and two features (the ratio of left pixels to right pixels and the ratio of right pixels to total pixels). The accuracy was still relatively low, which may be due to inconsistancies in the preprocessing algorithm, as the algorithm worked very well for some X-ray images and not well at all for others. With only 68.8% accuracy, there is still plenty of room for improvement in subsequent algorithms.
                  </p>
                <h4>KNN Evaluation</h4>
                  <ul>
                    <li>Accuracy Score: 0.68836</li>
                    <li>Precision Score: 0.67085</li>
                    <li>Recall Score: 0.72943</li>
                    <li>Jaccard Index: 0.53718</li>
                    <li>F1 Score (F-measure): 0.69891</li>
                  </ul>

              <h3>Label Spreading</h3>
                <h4>Preprocessing</h4>
                  <p>In this attempt, we used a different method for preprocessing our data than we used for KNN.  For Label Spreading, we read in the test images with Matplotlib's pyplot imread function.  We did not do much other preprocessing other than flattening each image to a one-dimensional numpy array.  Each pixel was considered a feature.  We used 70% of the data for training and 30% of the data for testing.
                  </p>
                <h4>Implementation</h4>
                  <p>Label Spreading is a true semi-supervised learning algorithm.  Label Spreading is a form of Label Propogation that uses a loss function and regularization to increase its capacity to handle noise.  We used the sci-kit learn implementation of the algorithm for the program.
                  </p>
                <h4>Results</h4>
                  <p>Label spreading was a significant improvement over any of the previous algorithms that we had tried.  At 86.8%, it boasted a fairly strong accuracy score, with both its Recall and F1 scores breaking 90%.  The full breakdown of evaluation scores can be seen below.
                <h4>Label Spreading Evaluation</h4>
                  <ul>
                    <li>Accuracy Score: 0.86782</li>
                    <li>Precision Score: 0.88208</li>
                    <li>Recall Score: 0.96321</li>
                    <li>Jaccard Index: 0.85333</li>
                    <li>F1 Score (F-measure): 0.92086</li>
                  </ul>


              <h3>Convolutional Neural Network (CNN)</h3>
                <h4>Preprocessing</h4>
                  <p>Similar to the Label Spreading Evaluation approach, we read the chest-Xrays images using Matplotlib's pyplot imread function. After this, we resized the array generated by the imread function to ensure that every image would have the same size arrays in order to be consistent with our 3-D numpy array that will be passed to the Neural Network.  Then, we split the data for training, 70%, and validation, 30%, so our model can learn and make modifications to the best CNN so far as the example shown on Figure x. Also, a testing data set is created using some of the images not used to choose the best model, which is 30 to 40% of the size of the training data set. It is important to emphasize that because the team decided to use the pytorch library, the data had to be converted from numpy arrays into a Tensor data set.
                  </p>
                  <img src="./images/accuracies_cnn2500.png"/>
                <h4>Implementation</h4>
                  <p>For Convolutional Neural Network, as previously mentioned, the team decided to use pytorch to build the structure of the model. This structure was composed of a combination of convolutional layers with their respective activation layers, such as ReLU and Max pooling, to then be passed to a linear regression layer to output the predictions. After having choosen the best model after 10 epochs and a batch-size of 32 for small data-sets, and 50 for bigger data-sets, our algorithm passes the algorithm on the testing data set.
                  </p>
                <h4>Results</h4>
                  <p>As seen from the results below, the accuracy of our model is below that one of the Label Spreading model, which can be explained by the nature of our data containing a significant bigger amount of positives for "Pheurial Effusion". The previous would make the CNN to generalize some of the "No Finding" images to be False Positives as shown in the confusion matrix shown below the results.

              <h4>Convolutional Neural Network Evaluation</h4>
                <ul>
                  <li>Accuracy Score: 0.84600</li>
                  <li>Precision Score: 0.86131</li>
                  <li>Recall Score: 0.95886</li>
                  <li>Jaccard Index: 0.83062</li>
                  <li>F1 Score (F-measure): 0.90747</li>
                </ul>
              <img src="./images/cm_cnn2500.png"/>

            </div>

            <h2>Conclusion</h2>
              <p>Overall, this project was a success, as both CNN and Label Spreading produced excellent results on the test data.  Through this project, we learned that it is often harder to prepare your data for machine learning than it is to actually perform the learning.  There are so many excellent algorithms freely available through libraries like sci-kit learn that the actual learning process is fairly simple to implement.  The challenging part can be getting the data from its original form into a form that can be read and understood by these algorithms.  While we could potentially have produced even better results with more advanced data pre-processing, we feel that we were able to successfully complete our goal of producing a solid algorithm for Pleural Effusion identification in X-Ray images. Therefore, we believe that the next steps to improve our prediction models should be focused on preprocessing the data using methods such as a Lung Identification algorith, and heatmaps.
              </p>
          </div>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>



  </body>
</html>
